{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate examples for the paper where the scores diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "from metrics_domain_adaptation import utils\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_all = pickle.load(open(f\"{utils.ROOT}/computed/scores_all_z.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model, domain, feature):\n",
    "    data = [x for lang in [\"en-de\", \"en-ru\", \"zh-en\"] for x in data_all[(domain, lang)]]\n",
    "    scores_model = np.array([x[\"scores\"][model]  for x in data])\n",
    "    scores_human = np.array([x[\"scores\"][\"human\"]  for x in data])\n",
    "\n",
    "    # compute loss and middle point\n",
    "    losses = np.abs(scores_human-scores_model)\n",
    "\n",
    "    losses_med_down = np.percentile(losses, q=25)\n",
    "    losses_med_up = np.percentile(losses, q=75)\n",
    "\n",
    "    scores_feature_high_loss = [feature(x) for x, l in zip(data, losses) if l >= losses_med_up]\n",
    "    scores_feature_low_loss = [feature(x) for x, l in zip(data, losses) if l <= losses_med_down]\n",
    "\n",
    "    return np.average(scores_feature_high_loss), np.average(scores_feature_low_loss)\n",
    "\n",
    "\n",
    "def format_cell(value, norm_min, norm_max):\n",
    "    value_color = (value-norm_min)/(norm_max-norm_min)*50\n",
    "    value_color = max(value_color, 0)\n",
    "    value_color = min(value_color, 45)\n",
    "    return f\"\\\\cellcolor{{black!{value_color:.2f}}} \"\n",
    "    \n",
    "\n",
    "def feature_importance_all(feature, name=\"\", precision=1):\n",
    "    print(f\"{name:>20}\", end=\" & \")\n",
    "    out = \"\"\n",
    "    for domain in [\"general\", \"bio\"]:\n",
    "        for model in [\"base\", \"ft\"]:\n",
    "            avg_up, avg_down = feature_importance(model, domain, feature)\n",
    "            txt_up = f\"{avg_up:.10f}\"[:-(10-precision)].removesuffix(\".\")\n",
    "            txt_down = f\"{avg_down:.10f}\"[:-(10-precision)].removesuffix(\".\")\n",
    "            diff = abs(avg_up-avg_down)\n",
    "            out += format_cell(diff/(0.01+min(abs(avg_up), abs(avg_down))), norm_min=0.0, norm_max=2) +  f\"{txt_up}/{txt_down} & \"\n",
    "    out = out.removesuffix(\"& \") + r\"\\\\\"\n",
    "    print(out)\n",
    "\n",
    "feature_importance_all(name=\"Target length\", precision=1, feature=lambda x: len(x[\"tgt\"].split()))\n",
    "feature_importance_all(name=\"Human score\", precision=1, feature=lambda x: x[\"scores\"][\"human\"])\n",
    "print(r\"\\\\[-0.7em]\")\n",
    "feature_importance_all(name=\"Critical sev.\", precision=2, feature=lambda x: len([i for i in x[\"errors\"][0] if i[\"severity\"] in {\"major\", \"critical\"}]))\n",
    "feature_importance_all(name=\"Minor severity\", precision=2, feature=lambda x: len([i for i in x[\"errors\"][0] if i[\"severity\"] == \"minor\"]))\n",
    "feature_importance_all(name=\"Neutral sev.\", precision=2, feature=lambda x: len([i for i in x[\"errors\"][0] if i[\"severity\"] == \"neutral\"]))\n",
    "print(r\"\\\\[-0.7em]\")\n",
    "feature_importance_all(name=\"Spelling cat.\", precision=2, feature=lambda x: len([i for i in x[\"errors\"][0] if i[\"category\"] == \"spelling\"]))\n",
    "feature_importance_all(name=\"Mistrans. cat.\", precision=2, feature=lambda x: len([i for i in x[\"errors\"][0] if i[\"category\"] == \"mistranslation\"]))\n",
    "feature_importance_all(name=\"Formatting cat.\", precision=2, feature=lambda x: len([i for i in x[\"errors\"][0] if i[\"category\"] == \"formatting\"]))\n",
    "feature_importance_all(name=\"Term. cat.\", precision=2, feature=lambda x: len([i for i in x[\"errors\"][0] if i[\"category\"] == \"terminology\"]))\n",
    "feature_importance_all(name=\"Grammar cat.\", precision=2, feature=lambda x: len([i for i in x[\"errors\"][0] if i[\"category\"] == \"grammar\"]))\n",
    "feature_importance_all(name=\"Fluency cat.\", precision=2, feature=lambda x: len([i for i in x[\"errors\"][0] if i[\"category\"] == \"fluency\"]))\n",
    "feature_importance_all(name=\"Other category\", precision=2, feature=lambda x: len([i for i in x[\"errors\"][0] if i[\"category\"] == \"other\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import scipy.stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "FEATURES = {\n",
    "    \"Target length\"   :lambda x: len(x[\"tgt\"].split()),\n",
    "    \"Human score\"     :lambda x: x[\"scores\"][\"human\"],\n",
    "    \"Critical sev.\"   :lambda x: len([i for i in x[\"errors\"][0] if i[\"severity\"] in {\"major\", \"critical\"}]),\n",
    "    \"Minor severity\"  :lambda x: len([i for i in x[\"errors\"][0] if i[\"severity\"] == \"minor\"]),\n",
    "    \"Neutral sev.\"    :lambda x: len([i for i in x[\"errors\"][0] if i[\"severity\"] == \"neutral\"]),\n",
    "\n",
    "    \"Fluency cat.\"     :lambda x: len([i for i in x[\"errors\"][0] if i[\"category\"] == \"fluency\"]),\n",
    "    \"Accuracy cat.\"    :lambda x: len([i for i in x[\"errors\"][0] if i[\"category\"] in {\"accuracy\", \"untranslated\"}]),\n",
    "    \"Terminology cat.\" :lambda x: len([i for i in x[\"errors\"][0] if i[\"category\"] == \"terminology\"]),\n",
    "    \"Locale cat.\"      :lambda x: len([i for i in x[\"errors\"][0] if i[\"category\"] == \"locale\"]),\n",
    "    \"Other category\"   :lambda x: len([i for i in x[\"errors\"][0] if i[\"category\"] == \"other\"]),\n",
    "}\n",
    "\n",
    "computed_features = {}\n",
    "\n",
    "def feature_importance_sm(model, domain):\n",
    "    data = [x for lang in [\"en-de\", \"en-ru\", \"zh-en\"] for x in data_all[(domain, lang)]]\n",
    "    scores_model = np.array([x[\"scores\"][model]  for x in data])\n",
    "    scores_human = np.array([x[\"scores\"][\"human\"]  for x in data])\n",
    "    losses = np.abs(scores_human-scores_model)\n",
    "    features = np.array([[feature(x) for feature in FEATURES.values()] for x in data])\n",
    "\n",
    "    features = StandardScaler().fit_transform(features)\n",
    "    losses = StandardScaler().fit_transform(losses.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    computed_features[(model, domain)] = {\n",
    "        f_name:scipy.stats.pearsonr(losses, features[:,f_i])\n",
    "        for f_i, f_name in enumerate(FEATURES)\n",
    "    }\n",
    "\n",
    "feature_importance_sm(\"base\", \"general\")\n",
    "feature_importance_sm(\"base\", \"bio\")\n",
    "feature_importance_sm(\"ft\", \"general\")\n",
    "feature_importance_sm(\"ft\", \"bio\")\n",
    "\n",
    "def format_cell(x, norm_min=0, norm_max=1):\n",
    "    coef, pval = x\n",
    "    value_color = (abs(coef)-norm_min)/(norm_max-norm_min)*50\n",
    "    value_color = max(value_color, 0)\n",
    "    value_color = min(value_color, 45)\n",
    "    if pval < 1e-5:\n",
    "        extra = r\"$^{*}$\"\n",
    "    else:\n",
    "        extra = r\"$^{\\hspace{1mm}}$\"\n",
    "\n",
    "    return f\"\\\\cellcolor{{black!{value_color:.2f}}} {coef:.2f} {extra}\"\n",
    "    \n",
    "for f_name in FEATURES:\n",
    "    if f_name in {\"Critical sev.\", \"Fluency cat.\"}:\n",
    "        print(r\"\\\\[-1em]\")\n",
    "    print(f_name + \" &\")\n",
    "    print(*[\n",
    "        format_cell(computed_features[(model, domain)][f_name])\n",
    "        for domain in [\"general\", \"bio\"]\n",
    "        for model in [\"base\", \"ft\"]\n",
    "    ], sep=\" & \", end=r\"\\\\\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find examples\n",
    "\n",
    "def format_line(line):\n",
    "    print(\n",
    "        f\"\\\\lsrc {line['src']} \\\\newline\\n\" +\n",
    "        f\"\\\\lmt {line['tgt']} \\\\newline\\n\" +\n",
    "        f\"\\\\lref {line['ref']} \\n\" +\n",
    "        f\"& \" +\n",
    "        f\"\\\\lhum {line['scores']['human']:.2f} \" +\n",
    "        f\"\\\\lbase {line['scores']['base']:.2f} \" +\n",
    "        f\"\\\\lft {line['scores']['ft']:.2f} \" +\n",
    "        \"\\\\\\\\\\n\"\n",
    "    )\n",
    "\n",
    "# en-de\n",
    "\n",
    "x = sorted(\n",
    "    data_all[(\"general\", \"en-de\")],\n",
    "    # we want high MQM score (good), high base score and low ft score\n",
    "    key=lambda x: -abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"base\"]) + x[\"scores\"][\"human\"],\n",
    "    reverse=True, # from highest to lowest\n",
    ")\n",
    "format_line(x[0])\n",
    "\n",
    "\n",
    "x = sorted(\n",
    "    [x for x in data_all[(\"general\", \"en-de\")] if len(x[\"src\"].split()) < 10],\n",
    "    key=lambda x: -abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"base\"]) - x[\"scores\"][\"human\"]*0.2,\n",
    "    reverse=True, # from highest to lowest\n",
    ")\n",
    "format_line(x[0])\n",
    "\n",
    "\n",
    "x = sorted(\n",
    "    [x for x in data_all[(\"bio\", \"en-de\")] if len(x[\"src\"].split()) < 10],\n",
    "    key=lambda x: -abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"ft\"]) + abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"base\"]),\n",
    "    reverse=True, # from highest to lowest\n",
    ")\n",
    "format_line(x[1])\n",
    "\n",
    "# zh-en\n",
    "\n",
    "x = sorted(\n",
    "    data_all[(\"general\", \"zh-en\")],\n",
    "    # we want high MQM score (good), high base score and low ft score\n",
    "    key=lambda x: -abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"base\"]) + x[\"scores\"][\"human\"]-abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"ft\"]),\n",
    "    reverse=True, # from highest to lowest\n",
    ")\n",
    "format_line(x[0])\n",
    "\n",
    "\n",
    "x = sorted(\n",
    "    [x for x in data_all[(\"general\", \"zh-en\")] if len(x[\"tgt\"].split()) < 10],\n",
    "    key=lambda x: -abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"base\"]) - x[\"scores\"][\"human\"]*0.2+abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"ft\"]),\n",
    "    reverse=True, # from highest to lowest\n",
    ")\n",
    "format_line(x[5])\n",
    "\n",
    "\n",
    "x = sorted(\n",
    "    [x for x in data_all[(\"bio\", \"zh-en\")] if len(x[\"tgt\"].split()) < 50 and x[\"scores\"][\"human\"]<0],\n",
    "    key=lambda x: -abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"ft\"])*3 + abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"base\"]),\n",
    "    reverse=True, # from highest to lowest\n",
    ")\n",
    "format_line(x[0])\n",
    "\n",
    "# ru-en\n",
    "\n",
    "x = sorted(\n",
    "    data_all[(\"general\", \"en-ru\")],\n",
    "    # we want high MQM score (good), high base score and low ft score\n",
    "    key=lambda x: -abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"base\"]) + x[\"scores\"][\"human\"]-abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"ft\"]),\n",
    "    reverse=True, # from highest to lowest\n",
    ")\n",
    "format_line(x[0])\n",
    "\n",
    "\n",
    "x = sorted(\n",
    "    [x for x in data_all[(\"general\", \"en-ru\")] if len(x[\"tgt\"].split()) < 10],\n",
    "    key=lambda x: -abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"base\"]) - x[\"scores\"][\"human\"]*0.2+abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"ft\"]),\n",
    "    reverse=True, # from highest to lowest\n",
    ")\n",
    "format_line(x[4])\n",
    "\n",
    "\n",
    "x = sorted(\n",
    "    [x for x in data_all[(\"bio\", \"en-ru\")] if len(x[\"tgt\"].split()) < 20 and x[\"scores\"][\"human\"]<0],\n",
    "    key=lambda x: -abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"ft\"])*2 + abs(x[\"scores\"][\"human\"] - x[\"scores\"][\"base\"]),\n",
    "    reverse=True, # from highest to lowest\n",
    ")\n",
    "format_line(x[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
